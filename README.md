# üáßüá∑ Olist End-to-End Data Pipeline

![Status](https://img.shields.io/badge/Status-Em_Desenvolvimento-yellow)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED)
![Spark](https://img.shields.io/badge/Apache_Spark-PySpark-E25A1C)

Um projeto pr√°tico de Engenharia de Dados simulando um ambiente de **Lakehouse** local.
O objetivo √© ingerir, processar e analisar dados reais do E-commerce brasileiro (Dataset p√∫blico da Olist), transformando dados brutos em insights de log√≠stica e vendas.

---

### üèóÔ∏è Arquitetura da Solu√ß√£o

O projeto segue a arquitetura **Medalh√£o (Multi-hop)** dentro de um ambiente containerizado:

```mermaid
graph LR
    A[Arquivos CSV] -->|Ingestion Script| B[(MinIO - Raw)]
    B -->|Spark - Cleaning| C[(MinIO - Silver)]
    C -->|Spark - Business Rules| D[(Postgres - Gold)]
    D -->|SQL/Analytics| E[Data Warehouse]
```
### üõ†Ô∏è Tech Stack
- Linguagem: Python 3.12
- Processamento: Apache Spark (PySpark)
- Armazenamento (Data Lake): MinIO (S3 Compatible) - Buckets: landing-zone, processing-zone
- Armazenamento (Data Warehouse): PostgreSQL
- Infraestrutura: Docker & Docker Compose
- Orquestra√ß√£o: Python Scripts (Pipeline sequencial)

### üîÑ Fluxo de Dados (Pipeline)
- Ingestion (Raw): Upload dos dados brutos (.csv) para o Object Storage simulando um Data Lake (S3).
- Silver Layer: Leitura dos dados brutos, limpeza de tipos de dados (casting de datas), remo√ß√£o de duplicatas e salvamento em formato colunar otimizado (Parquet).
- Gold Layer (Star Schema):
- Modelagem dimensional (Fatos e Dimens√µes).
- Cria√ß√£o de KPIs (Ex: C√°lculo de Delivery Delay).
- Enriquecimento: Join entre Vendedores e Geolocaliza√ß√£o (Lat/Long) para an√°lises espaciais.
- Carga final no Banco Relacional (Postgres).

### üöÄ Como Executar
#### Pr√©-requisitos
- Docker e Docker Compose instalados.
- Python 3.10+ (Recomendado uso de venv).

### Passo a Passo
#### Clone o reposit√≥rio:
```
git clone [https://github.com/SEU_USUARIO/lab-engenharia-dados.git](https://github.com/SEU_USUARIO/lab-engenharia-dados.git)
cd lab-engenharia-dados
```
#### Suba a infraestrutura:
```
docker-compose up -d
```
#### Prepare o ambiente Python:
```
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows
pip install -r requirements.txt
```
#### Execute o Pipeline:
```
python src/pipeline.py
```
### üìä Modelagem de Dados (Gold)
O Data Warehouse foi modelado em Star Schema para facilitar an√°lises de BI:

#### Fatos:
- fact_orders: Pedidos, datas e m√©tricas de atraso.
- fact_items: Itens vendidos e valores monet√°rios.

#### Dimens√µes:
- dim_customers: Dados dos clientes.
- dim_products: Dados dos produtos e categorias.
- dim_sellers: Dados dos vendedores com geolocaliza√ß√£o enriquecida.

### üìà Pr√≥ximos Passos (Roadmap)
- [x] Ingest√£o e Processamento Batch (Spark)
- [x] Modelagem Dimensional (Star Schema)
- [x] Enriquecimento de Dados (Geolocaliza√ß√£o)
##### Em Breve
- [ ] Dashboards no Metabase 
- [ ] Testes de Qualidade de Dados (Great Expectations)
- [ ] Orquestra√ß√£o com Apache Airflow

##### Desenvolvido por Pedro Vasconcelos [ppedro-vasco] como parte de um laborat√≥rio pr√°tico de Engenharia de Dados.
